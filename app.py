# app.py â€” KBS í•œêµ­ì–´ëŠ¥ë ¥ì‹œí—˜ RAG íŠœí„° (ì–´íœ˜/ê·œì •/ë‹¤ì˜ì–´ + íŒŒì¼ê¸°ë°˜ RAG + í€´ì¦ˆ)
import os
import json
import random
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from pypdf import PdfReader

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1) í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (.env ë˜ëŠ” Streamlit Secrets â†’ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•¨)
load_dotenv(override=True)

# 2) Streamlit ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="KBS í•œêµ­ì–´ëŠ¥ë ¥ì‹œí—˜ RAG íŠœí„°", layout="wide")
# ì œëª© (ë‘ ì¤„)
st.markdown(
    "<h1 style='text-align:center; line-height:1.3;'>"
    "âœ¨ğŸ˜ ì˜¤ë¡œì§€ ë‹¹ì‹ ë§Œì„ ìœ„í•œ~!<br>"
    "KBS í•œêµ­ì–´ëŠ¥ë ¥ì‹œí—˜ ìŒ¤ ğŸ’•ğŸ’«"
    "</h1>",
    unsafe_allow_html=True
)

# ì œëª©ê³¼ ì•ˆë‚´ë¬¸ ì‚¬ì´ 2ì¤„ ê³µë°±
st.markdown("<br><br>", unsafe_allow_html=True)

# ì•ˆë‚´ë¬¸ (ë‘ ì¤„ë¡œ ì¤„ë°”ê¿ˆ ì ìš©)
st.markdown(
    "<p style='text-align:center; color:#6b7280; font-size:14px; margin:10px 0 24px;'>"
    "ê·œì • ê·¼ê±°ëŠ” êµ­ë¦½êµ­ì–´ì›ì˜ ã€í•œê¸€ë§ì¶¤ë²•Â·í‘œì¤€ë°œìŒë²•Â·ì™¸ë˜ì–´Â·ë¡œë§ˆì í‘œê¸°ë²•ã€ë° "
    "ã€í‘œì¤€êµ­ì–´ëŒ€ì‚¬ì „ã€ì„ì„ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤."
    "</p>",
    unsafe_allow_html=True
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ë¡œë” (ì–´íœ˜/ê·œì •/ë‹¤ì˜ì–´)
@st.cache_data
def load_lexicon_df():
    """
    í†µí•© ì–´íœ˜ ë¡œë”: ê³ ìœ ì–´/ê´€ìš©êµ¬/ì†ë‹´/ì‚¬ìì„±ì–´/ìˆœí™”ì–´
    ê° CSV ìŠ¤í‚¤ë§ˆ: [ìœ í˜•, ì–´íœ˜, ëœ»í’€ì´] (ì˜ˆë¬¸, ë¹„ê³ ëŠ” ì„ íƒ)
    """
    files = [
        "data/ê³ ìœ ì–´.csv", "data/ê´€ìš©êµ¬.csv", "data/ì†ë‹´.csv",
        "data/ì‚¬ìì„±ì–´.csv", "data/ìˆœí™”ì–´.csv",
    ]
    cols_base = ["ìœ í˜•", "ì–´íœ˜", "ëœ»í’€ì´"]
    dfs = []
    for p in files:
        if os.path.exists(p):
            df = pd.read_csv(p)
            for c in cols_base:
                if c not in df.columns: df[c] = ""
            for c in ["ì˜ˆë¬¸", "ë¹„ê³ "]:
                if c not in df.columns: df[c] = ""
            dfs.append(df[cols_base + ["ì˜ˆë¬¸", "ë¹„ê³ "]])
    if not dfs:
        return pd.DataFrame(columns=cols_base + ["ì˜ˆë¬¸", "ë¹„ê³ "])
    out = pd.concat(dfs, ignore_index=True)
    for c in out.columns:
        out[c] = out[c].fillna("").astype(str)
    return out

@st.cache_data
def load_rules_list():
    # TODO: ê·œì • ë°ì´í„° ë¡œë”© í•¨ìˆ˜ êµ¬í˜„ ì˜ˆì •
    return []

@st.cache_data
def load_poly_df():
    """data/polysemy.csv (í‘œì œì–´,ì˜ë¯¸ë²ˆí˜¸,ëœ»,ì˜ˆë¬¸) â†’ í•­ìƒ DataFrame ë°˜í™˜"""
    path = "data/polysemy.csv"
    cols = ["í‘œì œì–´","ì˜ë¯¸ë²ˆí˜¸","ëœ»","ì˜ˆë¬¸"]
    if not os.path.exists(path):
        return pd.DataFrame(columns=cols)
    try:
        df = pd.read_csv(path)
    except Exception:
        return pd.DataFrame(columns=cols)
    # ëˆ„ë½ ì»¬ëŸ¼ ë³´ì •
    for c in cols:
        if c not in df.columns:
            df[c] = ""
    return df[cols].fillna("").astype(str)
    ...
# â† ì—¬ê¸°ë„ êµì²´
VOCAB = load_lexicon_df()
RULES = load_rules_list() or []
POLY  = load_poly_df()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# íŒŒì¼ ì—…ë¡œë“œ â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ ë²¡í„°DB êµ¬ì„± (ì—…ë¡œë“œì‹œë§Œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown(
    "<h5 style='margin-bottom: 0.3em;'>ğŸ§ ë³„ë„ë¡œ í•™ìŠµì‹œí‚¤ê³  ì‹¶ì€ ìë£Œê°€ ìˆìœ¼ì‹ ê°€ìš”? ì´ê³³ì— ì—…ë¡œë“œí•˜ì„¸ìš”! ğŸ‘‡ğŸ“‚</h5>",
    unsafe_allow_html=True
)
uploaded = st.file_uploader("txt ë˜ëŠ” pdf íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["txt", "pdf"])

def load_text(file):
    if file is None:
        return ""
    if file.type == "text/plain":
        return file.read().decode("utf-8", errors="ignore")
    if file.type == "application/pdf":
        reader = PdfReader(file)
        return "\n".join((page.extract_text() or "") for page in reader.pages)
    st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤.")
    st.stop()

text = ""
vectordb = None
retriever = None

if uploaded:
    text = load_text(uploaded)

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=200,
        separators=["\n\n","\n"," "]
    )
    docs = splitter.create_documents([text])

    with st.spinner("ì„ë² ë”©Â·ìƒ‰ì¸ êµ¬ì„± ì¤‘â€¦ (ìµœì´ˆ 1íšŒëŠ” ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ë‹¤ì†Œ ì†Œìš”)"):
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        vectordb = FAISS.from_documents(docs, embeddings)
        retriever = vectordb.as_retriever(search_kwargs={"k": 5})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LLM (ìƒì„±)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if not os.getenv("OPENAI_API_KEY"):
    st.warning("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šì•„ìš”. (Streamlit Secrets ë˜ëŠ” .env ì„¤ì • í•„ìš”)")
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

prompt = ChatPromptTemplate.from_template(
    "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•´ì„œ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”.\n"
    "ê°€ëŠ¥í•˜ë©´ ê·¼ê±°(ì˜ˆë¬¸/ê·œì •/ì¶œì „)ì„ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”.\n\n"
    "ì»¨í…ìŠ¤íŠ¸:\n{context}\n\n"
    "ì§ˆë¬¸: {question}"
)

def run_rag(question: str) -> str:
    """ì—…ë¡œë“œ ìë£Œ ê¸°ë°˜ RAG (ìë£Œ ì—†ìœ¼ë©´ ë¹ˆ ì»¨í…ìŠ¤íŠ¸)"""
    if retriever is None:
        context = ""
    else:
        docs = retriever.invoke(question)
        context = "\n\n".join(d.page_content for d in docs)
    chain = prompt | llm | StrOutputParser()
    return chain.invoke({"context": context, "question": question})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê°„ë‹¨ ì˜ë„ ë¶„ë¥˜ â†’ (ì–´íœ˜/ê·œì •/ë‹¤ì˜ì–´/íŒŒì¼ RAG) ë¼ìš°íŒ…
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def intent(text: str) -> str:
    t = text.strip()
    if any(k in t for k in ["ë§ì¶¤ë²•","ë„ì–´ì“°ê¸°","ë°œìŒ","ì™¸ë˜ì–´","ë¡œë§ˆì","í‘œê¸°","ì˜³ì€ í‘œê¸°","ë§ë‚˜ìš”"]):
        return "rule"
    if any(k in t for k in ["ë‹¤ì˜ì–´","ì—¬ëŸ¬ ëœ»","ëœ»ë“¤","ì˜ë¯¸ë“¤"]):
        return "poly"
    if any(k in t for k in ["ëœ»","ì˜ë¯¸","ì‚¬ìì„±ì–´","ì†ë‹´","ìœ ì˜ì–´","ê´€ìš©êµ¬","ë‹¨ì–´"]):
        return "vocab"
    return "rag"

def answer_vocab(q: str) -> str:
    if VOCAB.empty:
        return "ì‚¬ì „ ë°ì´í„°(ê³ ìœ ì–´Â·ê´€ìš©êµ¬Â·ì†ë‹´Â·ì‚¬ìì„±ì–´Â·ìˆœí™”ì–´ CSV)ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤."
    hit = VOCAB[VOCAB["ì–´íœ˜"].apply(lambda w: isinstance(w, str) and w in q)]
    if len(hit):
        row = hit.iloc[0]
        lines = [
            f"ã€”{row.get('ìœ í˜•','ì–´íœ˜')}ã€• {row.get('ì–´íœ˜','-')}",
            f"ëœ»: {row.get('ëœ»í’€ì´','-')}",
        ]
        ex = row.get("ì˜ˆë¬¸","")
        if isinstance(ex, str) and ex.strip():
            lines.append(f"ì˜ˆë¬¸: {ex}")
        extra = row.get("ë¹„ê³ ","")
        if isinstance(extra, str) and extra.strip():
            lines.append(f"ë¹„ê³ : {extra}")
        return "\n".join(lines)
    back = run_rag(f"ì–´íœ˜ ì˜ë¯¸: {q}")
    return "ì‚¬ì „ì— ì§ì ‘ ì¼ì¹˜í•˜ëŠ” ì–´íœ˜ê°€ ì—†ì–´ìš”.\n\n" + back


def answer_rule(q: str) -> str:
    """rules.jsonì—ì„œ í•­ëª© í‚¤ì›Œë“œ ë¶€ë¶„ì¼ì¹˜ ê²€ìƒ‰ (ì—¬ëŸ¬ ê°œë©´ ì²« í•­ëª©)"""
    if not RULES:
        return "ê·œì • ë°ì´í„°(rules.json)ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € data/rules.jsonì„ ì±„ì›Œ ì£¼ì„¸ìš”."
    candidates = []
    for item in RULES:
        text_blob = f"{item.get('í•­ëª©','')} {item.get('ì„¤ëª…','')}"
        if any(tok in text_blob for tok in q.split()):
            candidates.append(item)
    target = candidates[0] if candidates else (RULES[0] if RULES else None)
    if not target:
        return "í•´ë‹¹ ê·œì •ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”. ì§ˆë¬¸ì„ ì¡°ê¸ˆë§Œ ë‹¤ë¥´ê²Œ ì¨ë³¼ê¹Œìš”?"

    lines = [
        f"ã€”{target.get('ê·œì •ëª…','ê·œì •')}ã€• {target.get('í•­ëª©','')}",
        target.get('ì„¤ëª…',''),
    ]
    ex = target.get('ì˜ˆì‹œ','')
    if isinstance(ex, str) and ex.strip():
        lines.append(f"ì˜ˆì‹œ: {ex}")
    return "\n".join(lines)

def answer_poly(q: str) -> str:
    if POLY.empty:
        return "ë‹¤ì˜ì–´ ë°ì´í„°(polysemy.csv)ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤."
    cands = [w for w in POLY["í‘œì œì–´"].unique() if isinstance(w, str) and w in q]
    if not cands:
        return "ì–´ë–¤ ë‹¨ì–´ì˜ ì—¬ëŸ¬ ëœ»ì„ ë¬»ëŠ”ì§€ ì•Œë ¤ ì£¼ì„¸ìš”. (ì˜ˆ: 'ë“¤ë‹¤ ë‹¤ì˜ì–´ ì•Œë ¤ì¤˜')"
    w = max(cands, key=len)
    rows = POLY[POLY["í‘œì œì–´"] == w]
    lines = [f"â€˜{w}â€™ì˜ ëœ»"]
    for _, r in rows.sort_values("ì˜ë¯¸ë²ˆí˜¸").iterrows():
        num = int(r["ì˜ë¯¸ë²ˆí˜¸"]) if str(r["ì˜ë¯¸ë²ˆí˜¸"]).isdigit() else r["ì˜ë¯¸ë²ˆí˜¸"]
        lines.append(f" {num}. {r['ëœ»']}  (ì˜ˆ: {r['ì˜ˆë¬¸']})")
    return "\n".join(lines)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í€´ì¦ˆ ë¬¸í•­ ìƒì„±ê¸°: vocab.csvì—ì„œ në¬¸í•­ ë½‘ê¸° (ë©”íƒ€ëŠ” [ìœ í˜•]ë§Œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import re

def clean_text(s: str) -> str:
    """ë³´ê¸°/ì§ˆë¬¸ ì•ë’¤ì˜ íŠ¹ìˆ˜ë¬¸ìë‚˜ ìˆ«ì ì œê±°"""
    if not isinstance(s, str):
        return ""
    s = s.strip()
    # '=', '-', 'â‘¥', 'â‘ '~'â‘©', ê´„í˜¸, ìˆ«ì ë“± ì œê±°
    s = re.sub(r'^[=\-\dâ‘´-â‘½â‘´â‘½\(\)\sÂ·\.\,]+', '', s)
    s = re.sub(r'[\sÂ·\.\,]+$', '', s)
    return s.strip()

# ==== ê³µí†µ: CSV ì•ˆì „ ë¡œë” ====
def _read_csv_expect(path: str, expected_cols: list[str]) -> pd.DataFrame:
    if not os.path.exists(path):
        return pd.DataFrame(columns=expected_cols)
    df = pd.read_csv(path)
    for c in expected_cols:
        if c not in df.columns: df[c] = ""
    return df[expected_cols].fillna("").astype(str)

# ==== í†µí•© ì–´íœ˜ í€´ì¦ˆ ====
def build_quiz_lexicon(df: pd.DataFrame, n: int) -> list[dict]:
    need = {"ìœ í˜•","ì–´íœ˜","ëœ»í’€ì´"}
    if not need.issubset(df.columns) or df.empty: return []
    base = df.dropna(subset=["ì–´íœ˜","ëœ»í’€ì´"]).copy().sample(frac=1.0)
    items = []
    for _, r in base.iterrows():
        q_word, correct = r["ì–´íœ˜"].strip(), r["ëœ»í’€ì´"].strip()
        cat, ex = r.get("ìœ í˜•","ì–´íœ˜"), str(r.get("ì˜ˆë¬¸","")).strip()
        if not q_word or not correct: continue
        same = base[(base["ìœ í˜•"]==cat) & (base["ëœ»í’€ì´"]!=correct)]["ëœ»í’€ì´"].unique().tolist()
        if len(same) < 3:
            same = base[base["ëœ»í’€ì´"]!=correct]["ëœ»í’€ì´"].unique().tolist()
        if len(same) < 3: continue
        choices = random.sample(same, 3) + [correct]
        random.shuffle(choices)
        items.append({"question": f"â€˜{q_word}â€™ì˜ ëœ»ìœ¼ë¡œ ê°€ì¥ ì•Œë§ì€ ê²ƒì€?",
                      "choices": choices, "answer": correct, "ex": ex, "meta": f"[{cat}]"})
        if len(items) >= n: break
    return items

# ==== ë„ì–´ì“°ê¸° ====
def build_quiz_spacing(n: int) -> list[dict]:
    df = _read_csv_expect("data/ë„ì–´ì“°ê¸°.csv", ["ìœ í˜•","ì •ë‹µ","ì˜¤ë‹µ"])
    items = []
    if df.empty: return items
    for _, r in df.sample(frac=1.0).iterrows():
        wrong, correct, cat = r["ì˜¤ë‹µ"], r["ì •ë‹µ"], r.get("ìœ í˜•","ë„ì–´ì“°ê¸°")
        if not wrong or not correct: continue
        other = df[df["ì •ë‹µ"]!=correct]["ì •ë‹µ"].unique().tolist()
        if len(other) < 3: continue
        choices = random.sample(other, 3) + [correct]
        random.shuffle(choices)
        items.append({"question": f"â€˜{wrong}â€™ì˜ ì˜¬ë°”ë¥¸ ë„ì–´ì“°ê¸°ëŠ”?",
                      "choices": choices, "answer": correct, "ex": "", "meta": f"[{cat}]"})
        if len(items) >= n: break
    return items

# ==== ë§ì¶¤ë²• ====
def build_quiz_orthography(n: int) -> list[dict]:
    df = _read_csv_expect("data/ë§ì¶¤ë²•.csv", ["ìœ í˜•","ì •ë‹µ ë‹¨ì–´","ì˜¤ë‹µ ë‹¨ì–´"])
    items = []
    if df.empty: return items
    rights = df["ì •ë‹µ ë‹¨ì–´"].unique().tolist()
    for _, r in df.sample(frac=1.0).iterrows():
        wrong, correct, cat = r["ì˜¤ë‹µ ë‹¨ì–´"], r["ì •ë‹µ ë‹¨ì–´"], r.get("ìœ í˜•","ë§ì¶¤ë²•")
        if not wrong or not correct: continue
        other = [x for x in rights if x != correct]
        if len(other) < 3: continue
        choices = random.sample(other, 3) + [correct]
        random.shuffle(choices)
        items.append({"question": f"â€˜{wrong}â€™ì˜ ì˜¬ë°”ë¥¸ í‘œê¸°ëŠ”?",
                      "choices": choices, "answer": correct, "ex": "", "meta": f"[{cat}]"})
        if len(items) >= n: break
    return items

# ==== ì™¸ë˜ì–´ ====
def build_quiz_loanword(n: int) -> list[dict]:
    df = _read_csv_expect("data/ì™¸ë˜ì–´.csv", ["ìœ í˜•","ì •ë‹µ ì™¸ë˜ì–´","ì˜¤ë‹µ ì™¸ë˜ì–´"])
    items = []
    if df.empty: return items
    rights = df["ì •ë‹µ ì™¸ë˜ì–´"].unique().tolist()
    for _, r in df.sample(frac=1.0).iterrows():
        wrong, correct, cat = r["ì˜¤ë‹µ ì™¸ë˜ì–´"], r["ì •ë‹µ ì™¸ë˜ì–´"], r.get("ìœ í˜•","ì™¸ë˜ì–´")
        if not wrong or not correct: continue
        other = [x for x in rights if x != correct]
        if len(other) < 3: continue
        choices = random.sample(other, 2) + [wrong] + [correct]
        random.shuffle(choices)
        items.append({"question": "ë‹¤ìŒ ì¤‘ ì˜¬ë°”ë¥¸ ì™¸ë˜ì–´ í‘œê¸°ëŠ”?",
                      "choices": choices, "answer": correct, "ex": "", "meta": f"[{cat}]"})
        if len(items) >= n: break
    return items

# ==== í‘œì¤€ë°œìŒë²• ====
def build_quiz_pron(n: int) -> list[dict]:
    df = _read_csv_expect("data/í‘œì¤€ë°œìŒë²•.csv", ["ìœ í˜•","ë‹¨ì–´","í‘œì¤€ ë°œìŒ"])
    items = []
    if df.empty: return items
    for _, r in df.sample(frac=1.0).iterrows():
        word, correct, cat = r["ë‹¨ì–´"], r["í‘œì¤€ ë°œìŒ"], r.get("ìœ í˜•","í‘œì¤€ë°œìŒë²•")
        if not word or not correct: continue
        other = df[df["í‘œì¤€ ë°œìŒ"]!=correct]["í‘œì¤€ ë°œìŒ"].unique().tolist()
        if len(other) < 3: continue
        choices = random.sample(other, 3) + [correct]
        random.shuffle(choices)
        items.append({"question": f"â€˜{word}â€™ì˜ í‘œì¤€ ë°œìŒì€?",
                      "choices": choices, "answer": correct, "ex": "", "meta": f"[{cat}]"})
        if len(items) >= n: break
    return items

# ==== ë¡œë§ˆìí‘œê¸°ë²• ====
def build_quiz_romaja(n: int) -> list[dict]:
    df = _read_csv_expect("data/ë¡œë§ˆìí‘œê¸°ë²•.csv", ["ìœ í˜•","ë‹¨ì–´","ë¡œë§ˆì"])
    items = []
    if df.empty: return items
    for _, r in df.sample(frac=1.0).iterrows():
        word, correct, cat = r["ë‹¨ì–´"], r["ë¡œë§ˆì"], r.get("ìœ í˜•","ë¡œë§ˆìí‘œê¸°ë²•")
        if not word or not correct: continue
        other = df[df["ë¡œë§ˆì"]!=correct]["ë¡œë§ˆì"].unique().tolist()
        if len(other) < 3: continue
        choices = random.sample(other, 3) + [correct]
        random.shuffle(choices)
        items.append({"question": f"â€˜{word}â€™ì˜ ë¡œë§ˆì í‘œê¸°ëŠ”?",
                      "choices": choices, "answer": correct, "ex": "", "meta": f"[{cat}]"})
        if len(items) >= n: break
    return items

# ==== í‘œì¤€ì–´ê·œì • (ìœ í˜•/ë‹¨ì–´) ====
def build_quiz_standard_rule(n: int) -> list[dict]:
    df = _read_csv_expect("data/í‘œì¤€ì–´ê·œì •.csv", ["ìœ í˜•","ë‹¨ì–´"])
    items = []
    if df.empty: return items
    kinds = df["ìœ í˜•"].dropna().unique().tolist()
    if len(kinds) < 4 and kinds:
        while len(kinds) < 4: kinds.append(random.choice(kinds))
    for _, r in df.sample(frac=1.0).iterrows():
        word, correct = r["ë‹¨ì–´"], r["ìœ í˜•"]
        if not word or not correct: continue
        wrongs = [k for k in kinds if k != correct]
        if len(wrongs) < 3: continue
        choices = random.sample(wrongs, 3) + [correct]
        random.shuffle(choices)
        items.append({"question": f"â€˜{word}â€™ì€(ëŠ”) ì–´ë–¤ í‘œì¤€ì–´ ê·œì •ì— í•´ë‹¹í• ê¹Œìš”?",
                      "choices": choices, "answer": correct, "ex": "", "meta": "[í‘œì¤€ì–´ê·œì •]"})
        if len(items) >= n: break
    return items

# ==== ì „ì²´ í˜¼í•© í€´ì¦ˆ ====
def build_all_quiz_items(total: int = 10) -> list[dict]:
    per = max(1, total // 6)   # ëŒ€ëµ ê· ë“±
    items = []
    items += build_quiz_lexicon(VOCAB, n=per*2)   # ì–´íœ˜ ë¹„ì¤‘ ì¡°ê¸ˆ ë”
    items += build_quiz_spacing(per)
    items += build_quiz_orthography(per)
    items += build_quiz_loanword(per)
    items += build_quiz_pron(per)
    items += build_quiz_romaja(per)
    items += build_quiz_standard_rule(per)
    random.shuffle(items)
    return items[:total]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# íƒ­ UI: ì§ˆë¬¸í•˜ê¸° | í€´ì¦ˆ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tab_ask, tab_quiz = st.tabs(["ğŸ§ ì§ˆë¬¸í•˜ê¸°", "ğŸ¤— í€´ì¦ˆ í’€ê¸°"])

with tab_ask:
    # ë¼ë²¨ ë¶€ë¶„ì„ HTMLë¡œ ì§ì ‘ ì¶œë ¥ (ì—”í„° í¬í•¨)
    st.markdown(
        "<p style='font-size:18px; font-weight:600;'>ğŸ„ ì €ì—ê²Œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš© ğŸ²!</p>"
        "<p style='font-size:14px; color:#555; margin-top:-10px; margin-bottom:-8px;'>"
        "(e.g. 'êµê°ì‚´ìš°'ì˜ ëœ»ì´ ê¶ê¸ˆí•´ìš”,<br>"
        "'ëŠ‘ë§‰ì—¼'ì˜ í‘œì¤€ ë°œìŒì„ ì•Œë ¤ì£¼ì„¸ìš”,<br>"
        "ìë£Œì—ì„œ ~ ëŠ” ì–´ë””ì— ë‚˜ì˜¤ë‚˜ìš”?)"
        "</p>",
        unsafe_allow_html=True
    )

    # ì…ë ¥ì°½ (ì˜ˆì‹œ ë¬¸êµ¬ì™€ ë”± ë¶™ê²Œ)
    user_q = st.text_input("", placeholder="ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” ğŸ˜Š")

    # ì‘ì› ë¬¸êµ¬ (2ì¤„ ê³µë°± í›„)
    st.markdown(
        "<br><br>"
        "<p style='color:gray; font-size:20px; font-weight:600; text-align:center;'>"
        "íŒŒì´íŒ…!!! ìŒ¤ì€ ì—¬ëŸ¬ë¶„ì˜ ì‹œí—˜ì„ ì‘ì›í•©ë‹ˆë‹¤~! ğŸ¥°"
        "</p>",
        unsafe_allow_html=True
    )

    if user_q:
        kind = intent(user_q)
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘â€¦"):
            try:
                if kind == "vocab":
                    ans = answer_vocab(user_q)
                elif kind == "rule":
                    ans = answer_rule(user_q)
                elif kind == "poly":
                    ans = answer_poly(user_q)
                else:
                    ans = run_rag(user_q)

                st.write(ans)

                if kind == "rag" and retriever is not None:
                    with st.expander("ğŸ” ê·¼ê±° ë³´ê¸°(ì—…ë¡œë“œ ìë£Œì—ì„œ ì¶”ì¶œ)"):
                        docs = retriever.invoke(user_q)
                        for i, d in enumerate(docs, 1):
                            st.markdown(f"**ê·¼ê±° {i}**")
                            st.code((d.page_content or "")[:800])
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í€´ì¦ˆ íƒ­ (í™•ì¥ ë²„ì „: ì œì¶œ â†’ ê²°ê³¼ â†’ ìƒˆ í€´ì¦ˆ ë²„íŠ¼ ìˆœì„œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_quiz:
    st.markdown("â¤ï¸ì¨”ë€! **ëœë¤ ì¢…í•© í€´ì¦ˆ**ë¥¼ í’€ì–´ë³´ì„¸ìš”!ğŸ˜˜")

    # ì²˜ìŒ ë¡œë“œ ì‹œ data/ ëª¨ë“  CSV ê¸°ë°˜ìœ¼ë¡œ í˜¼í•© í€´ì¦ˆ ìƒì„±
    if "quiz_items" not in st.session_state or not st.session_state.quiz_items:
        st.session_state.quiz_items = build_all_quiz_items(total=10)  # ì´ ë¬¸í•­ ìˆ˜ ì¡°ì ˆ ê°€ëŠ¥
        st.session_state.quiz_submitted = False
        st.session_state.quiz_score = 0

    if not st.session_state.quiz_items:
        st.info("í€´ì¦ˆë¥¼ ë§Œë“¤ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. data/ í´ë”ì˜ CSVë“¤ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
    else:
        answers = {}
        for i, item in enumerate(st.session_state.quiz_items):
            st.markdown(
                f"**Q{i+1}. {item['question']}**  \n<small>{item['meta']}</small>",
                unsafe_allow_html=True
            )
            key = f"quiz_q_{i}"
            choice = st.radio("ë³´ê¸°", options=item["choices"], index=None, key=key, label_visibility="collapsed")
            answers[i] = choice
            st.divider()
        # ì œì¶œ/ì±„ì /í•´ì„¤ ë¶€ë¶„ì€ ê¸°ì¡´ ì½”ë“œ ìœ ì§€


        # âœ… ì œì¶œ ë²„íŠ¼ (ë§¨ ì•„ë˜)
        if st.button("âœ… ì œì¶œ", type="primary", use_container_width=True):
            score = 0
            results = []
            for i, item in enumerate(st.session_state.quiz_items):
                sel = answers.get(i)
                ok = (sel == item["answer"])
                score += int(ok)
                results.append((ok, sel, item))
            st.session_state.quiz_score = score
            st.session_state.quiz_submitted = True

            st.success(f"ì ìˆ˜: **{score} / {len(st.session_state.quiz_items)}**")
            with st.expander("ì •ë‹µ ë° í•´ì„¤ ë³´ê¸°"):
                for i, (ok, sel, item) in enumerate(results, start=1):
                    icon = "âœ…" if ok else "âŒ"
                    sel_txt = sel if sel is not None else "(ë¬´ì‘ë‹µ)"
                    st.markdown(f"**{icon} Q{i}. {item['question']}**")
                    st.write(f"- ì„ íƒ: {sel_txt}")
                    st.write(f"- ì •ë‹µ: {item['answer']}")
                    if item["ex"]:
                        st.write(f"- ì˜ˆë¬¸: {item['ex']}")
                    st.write("---")

        # ğŸ”„ ìƒˆ í€´ì¦ˆ ì¶œì œ ë²„íŠ¼ (ì œì¶œ ì•„ë˜)
        if st.button("ğŸ”„ ìƒˆ í€´ì¦ˆ ì¶œì œ", use_container_width=True):
           st.session_state.quiz_items = build_all_quiz_items(total=10)
           st.session_state.quiz_submitted = False
           st.session_state.quiz_score = 0
           st.rerun()

                    
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ì´ë“œë°”: ìƒíƒœ/í™•ì¥ ì•ˆë‚´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.markdown("### ìƒíƒœ")
    st.write(f"- ì–´íœ˜ ì‚¬ì „ ë¡œë“œ: {'âœ…' if not VOCAB.empty else 'âŒ'}")
    st.write(f"- ê·œì • ì¹´ë“œ ë¡œë“œ: {'âœ…' if RULES else 'âŒ'}")
    # ê¸°ì¡´: st.write(f"- ë‹¤ì˜ì–´ ì¹´ë“œ ë¡œë“œ: {'âœ…' if not POLY.empty else 'âŒ'}")
    poly_ok = isinstance(POLY, pd.DataFrame) and not POLY.empty
    st.write(f"- ë‹¤ì˜ì–´ ì¹´ë“œ ë¡œë“œ: {'âœ…' if poly_ok else 'âŒ'}")
    st.write(f"- ì—…ë¡œë“œ ìë£Œ ìƒ‰ì¸: {'âœ…' if retriever is not None else 'âŒ'}")
    st.divider()
    st.markdown("### ì‚¬ìš©ë²•")
    st.markdown("- ì–´íœ˜: `êµê°ì‚´ìš° ëœ»`, `ì„ì”¨ë…„ìŠ¤ëŸ½ë‹¤ ì˜ë¯¸`")
    st.markdown("- ê·œì •: `ê°™ì´ ë„ì–´ì“°ê¸°`, `ê°’ì´ ë°œìŒ`, `í”¼ì í‘œê¸°`")
    st.markdown("- ë‹¤ì˜ì–´: `ë“¤ë‹¤ ë‹¤ì˜ì–´`, `ë‹¬ë‹¤ ì—¬ëŸ¬ ëœ»`, `ì¹˜ë¥´ë‹¤ ëœ»ë“¤`")
    st.markdown("- í€´ì¦ˆ: íƒ­ì—ì„œ **ìƒˆ í€´ì¦ˆ ì¶œì œ â†’ ì œì¶œ**")
    st.markdown("- ì—…ë¡œë“œ RAG: íŒŒì¼ ì˜¬ë¦¬ê³  ììœ  ì§ˆì˜")






































