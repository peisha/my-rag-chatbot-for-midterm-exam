import os
import streamlit as st
from dotenv import load_dotenv
from pypdf import PdfReader

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1) .env ë¡œë“œ (Downloads í´ë”ì˜ .env)
load_dotenv(override=True)

# 2) Streamlit ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="My RAG Chatbot")
st.title("ğŸ“˜ íŒŒì¼ ì—…ë¡œë“œí˜• RAG ì±—ë´‡")
st.caption("txt ë˜ëŠ” pdf íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”")

# 3) íŒŒì¼ ì—…ë¡œë”
uploaded = st.file_uploader("íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type=["txt", "pdf"])

# 4) íŒŒì¼ í…ìŠ¤íŠ¸ ë¡œë”© í•¨ìˆ˜
def load_text(file):
    if file is None:
        return ""
    if file.type == "text/plain":
        return file.read().decode("utf-8", errors="ignore")
    if file.type == "application/pdf":
        reader = PdfReader(file)
        return "\n".join((page.extract_text() or "") for page in reader.pages)
    st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤.")
    st.stop()

# 5) ì—…ë¡œë“œ íŒŒì¼ ì²˜ë¦¬
if uploaded:
    text = load_text(uploaded)

    # (a) í…ìŠ¤íŠ¸ ë¶„í• 
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs = splitter.create_documents([text])

    # (b) ì„ë² ë”©: ë¹„ìš© ì—†ëŠ” ë¡œì»¬ ì„ë² ë”© ì‚¬ìš©
    #  - ëª¨ë¸ ë‹¤ìš´ë¡œë“œëŠ” ìµœì´ˆ 1íšŒë§Œ ì´ë£¨ì–´ì§€ê³ , ì´í›„ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    with st.spinner("ì„ë² ë”© ìƒì„± ì¤‘â€¦ (ìµœì´ˆ 1íšŒëŠ” ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”)"):
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        vectordb = FAISS.from_documents(docs, embeddings)

    # (c) ë¦¬íŠ¸ë¦¬ë²„
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})

    # (d) LLM: OpenAI ì‚¬ìš© (ì‘ë‹µ ìƒì„±ìš©)
    if not os.getenv("OPENAI_API_KEY"):
        st.error("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. (.env íŒŒì¼ í™•ì¸)")
        st.stop()

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    # (e) í”„ë¡¬í”„íŠ¸
    prompt = ChatPromptTemplate.from_template(
        "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•´ì„œ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”.\n"
        "í•„ìš”í•˜ë©´ ê·¼ê±°ë¥¼ ìš”ì•½í•´ì„œ ë§ë¶™ì´ì„¸ìš”.\n\n"
        "ì»¨í…ìŠ¤íŠ¸:\n{context}\n\n"
        "ì§ˆë¬¸: {question}"
    )

    # (f) RAG ì²´ì¸ ì‹¤í–‰ í•¨ìˆ˜
    def run_rag(question: str) -> str:
        docs = retriever.invoke(question)
        context = "\n\n".join(d.page_content for d in docs)
        chain = prompt | llm | StrOutputParser()
        return chain.invoke({"context": context, "question": question})

    # (g) ì§ˆë¬¸ ì…ë ¥ UI
    query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
    if query:
        with st.spinner("ê²€ìƒ‰/ìƒì„± ì¤‘â€¦"):
            try:
                answer = run_rag(query)
                st.write("ğŸ’¬ ë‹µë³€:", answer)
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
else:
    st.info("ğŸ“‚ ìƒë‹¨ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ì¤€ë¹„ê°€ ì™„ë£Œë©ë‹ˆë‹¤.")
