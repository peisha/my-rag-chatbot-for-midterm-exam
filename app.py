# app.py â€” KBS í•œêµ­ì–´ëŠ¥ë ¥ì‹œí—˜ RAG íŠœí„° (ì–´íœ˜/ê·œì •/ë‹¤ì˜ì–´ + íŒŒì¼ê¸°ë°˜ RAG + í€´ì¦ˆ)
import os
import json
import random
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from pypdf import PdfReader

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1) í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (.env ë˜ëŠ” Streamlit Secrets â†’ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•¨)
load_dotenv(override=True)

# 2) Streamlit ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="KBS í•œêµ­ì–´ëŠ¥ë ¥ì‹œí—˜ RAG íŠœí„°", layout="wide")
# ì œëª© (ë‘ ì¤„)
st.markdown(
    "<h1 style='text-align:center; line-height:1.3;'>"
    "âœ¨ğŸ˜ ì˜¤ë¡œì§€ ë‹¹ì‹ ë§Œì„ ìœ„í•œ~!<br>"
    "KBS í•œêµ­ì–´ëŠ¥ë ¥ì‹œí—˜ ìŒ¤ ğŸ’•ğŸ’«"
    "</h1>",
    unsafe_allow_html=True
)

# ì œëª©ê³¼ ì•ˆë‚´ë¬¸ ì‚¬ì´ 1ì¤„ ê³µë°±
st.markdown("<br>", unsafe_allow_html=True)

# ì•ˆë‚´ë¬¸ (ë‘ ì¤„ë¡œ ì¤„ë°”ê¿ˆ ì ìš©)
st.markdown(
    "<p style='text-align:center; color:#6b7280; font-size:14px; margin:10px 0 24px;'>"
    "ê·œì • ê·¼ê±°ëŠ” êµ­ë¦½êµ­ì–´ì›ì˜ 'í•œê¸€ë§ì¶¤ë²•Â·í‘œì¤€ë°œìŒë²•Â·ì™¸ë˜ì–´Â·ë¡œë§ˆì í‘œê¸°ë²•' ë° "
    "'í‘œì¤€êµ­ì–´ëŒ€ì‚¬ì „'ì— ìˆìŠµë‹ˆë‹¤."
    "</p>",
    unsafe_allow_html=True
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ë¡œë” (ì–´íœ˜/ê·œì •/ë‹¤ì˜ì–´)
@st.cache_data
def load_lexicon_df():
    """
    í†µí•© ì–´íœ˜ ë¡œë”: ê³ ìœ ì–´/ê´€ìš©êµ¬/ì†ë‹´/ì‚¬ìì„±ì–´/ìˆœí™”ì–´
    ê° CSV ìŠ¤í‚¤ë§ˆ: [ìœ í˜•, ì–´íœ˜, ëœ»í’€ì´] (ì˜ˆë¬¸, ë¹„ê³ ëŠ” ì„ íƒ)
    """
    files = [
        "data/ê³ ìœ ì–´.csv", "data/ê´€ìš©êµ¬.csv", "data/ì†ë‹´.csv",
        "data/ì‚¬ìì„±ì–´.csv", "data/ìˆœí™”ì–´.csv",
    ]
    cols_base = ["ìœ í˜•", "ì–´íœ˜", "ëœ»í’€ì´"]
    dfs = []
    for p in files:
        if os.path.exists(p):
            df = pd.read_csv(p)
            for c in cols_base:
                if c not in df.columns: df[c] = ""
            for c in ["ì˜ˆë¬¸", "ë¹„ê³ "]:
                if c not in df.columns: df[c] = ""
            dfs.append(df[cols_base + ["ì˜ˆë¬¸", "ë¹„ê³ "]])
    if not dfs:
        return pd.DataFrame(columns=cols_base + ["ì˜ˆë¬¸", "ë¹„ê³ "])
    out = pd.concat(dfs, ignore_index=True)
    for c in out.columns:
        out[c] = out[c].fillna("").astype(str)
    return out

@st.cache_data
def load_rules_list():
    # TODO: ê·œì • ë°ì´í„° ë¡œë”© í•¨ìˆ˜ êµ¬í˜„ ì˜ˆì •
    return []

@st.cache_data
def load_poly_df():
    """data/polysemy.csv (í‘œì œì–´,ì˜ë¯¸ë²ˆí˜¸,ëœ»,ì˜ˆë¬¸) â†’ í•­ìƒ DataFrame ë°˜í™˜"""
    path = "data/polysemy.csv"
    cols = ["í‘œì œì–´","ì˜ë¯¸ë²ˆí˜¸","ëœ»","ì˜ˆë¬¸"]
    if not os.path.exists(path):
        return pd.DataFrame(columns=cols)
    try:
        df = pd.read_csv(path)
    except Exception:
        return pd.DataFrame(columns=cols)
    # ëˆ„ë½ ì»¬ëŸ¼ ë³´ì •
    for c in cols:
        if c not in df.columns:
            df[c] = ""
    return df[cols].fillna("").astype(str)

# CSV íŒŒì¼ ë¡œë“œ
VOCAB = load_lexicon_df()
RULES = load_rules_list() or []
POLY = load_poly_df()

# ==========================
# 1) ê·œì • JSON ë¡œë“œ & ë¬¸ì„œí™”
# ==========================
# ìƒë‹¨ import ê·¼ì²˜
import json, os
from langchain_core.documents import Document
from typing import List
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

RULES_JSON_PATH = "rules.json"  # ì—…ë¡œë“œë¡œ ë®ì–´ì“¸ íŒŒì¼ëª…

@st.cache_data(show_spinner=False)
def load_rule_docs(path: str = RULES_JSON_PATH) -> list[Document]:
    """rules.json â†’ LangChain Document ëª©ë¡. íŒŒì¼ ì—†ê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨ ì‹œ []"""
    if not os.path.exists(path):
        return []
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception:
        return []

    docs: list[Document] = []
    for row in data:
        j = row.get
        ì¥, ì ˆ, í•­, ì œëª© = j("ì¥",""), j("ì ˆ",""), j("í•­",""), j("ì œëª©","")
        ì„¤ëª… = j("ì„¤ëª…","")
        ok   = ", ".join(j("ì˜ˆì‹œ_ì˜³ìŒ",  []))
        bad  = ", ".join(j("ì˜ˆì‹œ_í‹€ë¦¼", []))
        excp = ", ".join(j("ì˜ˆì‹œ_ì˜ˆì™¸",  []))
        body = [ì„¤ëª…]
        if ok:  body.append(f"[ì˜ˆì‹œ_ì˜³ìŒ] {ok}")
        if bad: body.append(f"[ì˜ˆì‹œ_í‹€ë¦¼] {bad}")
        if excp: body.append(f"[ì˜ˆì‹œ_ì˜ˆì™¸] {excp}")
        text = f"{ì¥} Â· {ì ˆ} Â· {í•­}\n{ì œëª©}\n\n" + "\n".join(body)
        docs.append(Document(page_content=text, metadata={"ì¥": ì¥, "ì ˆ": ì ˆ, "í•­": í•­, "ì œëª©": ì œëª©}))
    return docs

# =========================
# 2) ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•/ë¡œë“œ
# =========================
@st.cache_resource(show_spinner=False)
def build_rule_retriever(docs: List[Document]):
    embed = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vs = FAISS.from_documents(docs, embed)
    return vs.as_retriever(search_kwargs={"k": 4})

# ë¶€íŒ… ì‹œ ê·œì • ìƒ‰ì¸ ì¤€ë¹„ (íŒŒì¼ ì—†ìœ¼ë©´ None)
rule_docs = load_rule_docs(RULES_JSON_PATH)
retriever_rules = build_rule_retriever(rule_docs) if rule_docs else None

# =========================
# 3) ê·œì • Q&A í•¨ìˆ˜
# =========================
def answer_rule(user_q: str) -> str:
    """ê·œì • ì „ìš© ì§ˆì˜ ì‘ë‹µ: ìµœìƒìœ„ ê·¼ê±° 1~2ê°œë¥¼ ìš”ì•½í•´ ë³´ì—¬ì¤Œ"""
    hits = rule_retriever.invoke(user_q)
    if not hits:
        return "ê·œì •ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”. ì§ˆë¬¸ì„ ì¡°ê¸ˆë§Œ ë°”ê¿”ì„œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”!"

    # ìµœìƒìœ„ ê²°ê³¼ë¡œ ê°„ë‹¨í•œ ë‹µë³€ + ë©”íƒ€(ì¥/ì ˆ/í•­/ì œëª©)ì™€ ê·¼ê±° ì¼ë¶€ë¥¼ ë³´ì—¬ì¤Œ
    top = hits[0]
    meta = top.metadata
    header = f"**{meta.get('ì¥','')} Â· {meta.get('ì ˆ','')} Â· {meta.get('í•­','')}** â€” {meta.get('ì œëª©','')}"
    snippet = top.page_content[:800]  # ë„ˆë¬´ ê¸¸ë©´ 800ìê¹Œì§€ë§Œ
    out = f"{header}\n\n{snippet}"

    # ì¶”ê°€ ê·¼ê±°(ì„ íƒ)
    if len(hits) > 1:
        out += "\n\n---\n**ì¶”ê°€ ê·¼ê±°**\n"
        for h in hits[1:3]:
            m = h.metadata
            out += f"- {m.get('ì¥','')} {m.get('ì ˆ','')} {m.get('í•­','')} â€” {m.get('ì œëª©','')}\n"

    return out

# =========================
# 4) (ì„ íƒ) ì•±ì—ì„œ íŒŒì¼ ì—…ë¡œë“œë¡œ êµì²´ ê°€ëŠ¥
# =========================
with st.expander("ğŸ“¤ ê·œì • JSON ì—…ë¡œë“œ (ì„ íƒ)"):
    up = st.file_uploader("rules.json ì—…ë¡œë“œ", type=["json"])
    if up is not None:
        with open(RULES_JSON_PATH, "wb") as f:
            f.write(up.read())
        st.cache_data.clear()
        st.cache_resource.clear()
        st.success("ê·œì • ë°ì´í„°ê°€ ê°±ì‹ ë˜ì—ˆì–´ìš”. ì ì‹œ í›„ ìë™ìœ¼ë¡œ ë°˜ì˜ë©ë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# íŒŒì¼ ì—…ë¡œë“œ â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ ë²¡í„°DB êµ¬ì„± (ì—…ë¡œë“œì‹œë§Œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown(
    """
    <p style='text-align:center; font-size:15px; color:#4b5563; margin-top:10px; margin-bottom:14px;'>
        ğŸ“‚ ë³„ë„ë¡œ í•™ìŠµì‹œí‚¤ê³  ì‹¶ì€ ìë£Œê°€ ìˆìœ¼ì‹œë‹¤ë©´<br>
        <span style='color:#111827; font-weight:600;'>í•˜ë‹¨ì— ì²¨ë¶€í•´ì£¼ì„¸ìš”.</span>
    </p>
    """,
    unsafe_allow_html=True
)

uploaded = st.file_uploader("ì´ê³³ì— txt ë˜ëŠ” pdf íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["txt", "pdf"])

def load_text(file):
    if file is None:
        return ""
    if file.type == "text/plain":
        return file.read().decode("utf-8", errors="ignore")
    if file.type == "application/pdf":
        reader = PdfReader(file)
        return "\n".join((page.extract_text() or "") for page in reader.pages)
    st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤.")
    st.stop()

text = ""
vectordb = None
retriever = None

if uploaded:
    text = load_text(uploaded)

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=200,
        separators=["\n\n","\n"," "]
    )
    docs = splitter.create_documents([text])

    with st.spinner("ì„ë² ë”©Â·ìƒ‰ì¸ êµ¬ì„± ì¤‘â€¦ (ìµœì´ˆ 1íšŒëŠ” ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ë‹¤ì†Œ ì†Œìš”)"):
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        vectordb = FAISS.from_documents(docs, embeddings)
        retriever = vectordb.as_retriever(search_kwargs={"k": 5})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LLM (ìƒì„±)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if not os.getenv("OPENAI_API_KEY"):
    st.warning("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šì•„ìš”. (Streamlit Secrets ë˜ëŠ” .env ì„¤ì • í•„ìš”)")
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

prompt = ChatPromptTemplate.from_template(
    "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•´ì„œ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”.\n"
    "ê°€ëŠ¥í•˜ë©´ ê·¼ê±°(ì˜ˆë¬¸/ê·œì •/ì¶œì „)ì„ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”.\n\n"
    "ì»¨í…ìŠ¤íŠ¸:\n{context}\n\n"
    "ì§ˆë¬¸: {question}"
)

def run_rag(question: str) -> str:
    """ì—…ë¡œë“œ ìë£Œ ê¸°ë°˜ RAG (ìë£Œ ì—†ìœ¼ë©´ ë¹ˆ ì»¨í…ìŠ¤íŠ¸)"""
    if retriever is None:
        context = ""
    else:
        docs = retriever.invoke(question)
        context = "\n\n".join(d.page_content for d in docs)
    chain = prompt | llm | StrOutputParser()
    return chain.invoke({"context": context, "question": question})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê°„ë‹¨ ì˜ë„ ë¶„ë¥˜ â†’ (ì–´íœ˜/ê·œì •/ë‹¤ì˜ì–´/íŒŒì¼ RAG) ë¼ìš°íŒ…
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def intent(text: str) -> str:
    t = text.strip()
    if any(k in t for k in ["ë§ì¶¤ë²•","ë„ì–´ì“°ê¸°","ë°œìŒ","ì™¸ë˜ì–´","ë¡œë§ˆì","í‘œê¸°","ì˜³ì€ í‘œê¸°","ë§ë‚˜ìš”"]):
        return "rule"
    if any(k in t for k in ["ë‹¤ì˜ì–´","ì—¬ëŸ¬ ëœ»","ëœ»ë“¤","ì˜ë¯¸ë“¤"]):
        return "poly"
    if any(k in t for k in ["ëœ»","ì˜ë¯¸","ì‚¬ìì„±ì–´","ì†ë‹´","ìœ ì˜ì–´","ê´€ìš©êµ¬","ë‹¨ì–´"]):
        return "vocab"
    return "rag"

def answer_vocab(q: str) -> str:
    if VOCAB.empty:
        return "ì‚¬ì „ ë°ì´í„°(ê³ ìœ ì–´Â·ê´€ìš©êµ¬Â·ì†ë‹´Â·ì‚¬ìì„±ì–´Â·ìˆœí™”ì–´ CSV)ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤."
    hit = VOCAB[VOCAB["ì–´íœ˜"].apply(lambda w: isinstance(w, str) and w in q)]
    if len(hit):
        row = hit.iloc[0]
        lines = [
            f"ã€”{row.get('ìœ í˜•','ì–´íœ˜')}ã€• {row.get('ì–´íœ˜','-')}",
            f"ëœ»: {row.get('ëœ»í’€ì´','-')}",
        ]
        ex = row.get("ì˜ˆë¬¸","")
        if isinstance(ex, str) and ex.strip():
            lines.append(f"ì˜ˆë¬¸: {ex}")
        extra = row.get("ë¹„ê³ ","")
        if isinstance(extra, str) and extra.strip():
            lines.append(f"ë¹„ê³ : {extra}")
        return "\n".join(lines)
    back = run_rag(f"ì–´íœ˜ ì˜ë¯¸: {q}")
    return "ì‚¬ì „ì— ì§ì ‘ ì¼ì¹˜í•˜ëŠ” ì–´íœ˜ê°€ ì—†ì–´ìš”.\n\n" + back


def answer_rule(q: str):
    """ê·œì • ì „ìš© RAG: rules.json ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰"""
    if retriever_rules is None:
        return "ê·œì • ë°ì´í„°ê°€ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ì–´ìš”. rules.jsonì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”!"

    hits = retriever_rules.invoke(q)
    if not hits:
        return "ê´€ë ¨ ê·œì •ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ ì¨ ë³¼ê¹Œìš”?"

    top = hits[0]
    meta = top.metadata
    header = f"**{meta.get('ì¥','')} Â· {meta.get('ì ˆ','')} Â· {meta.get('í•­','')} â€” {meta.get('ì œëª©','')}**"
    snippet = top.page_content[:800]
    out = f"{header}\n\n{snippet}"

    if len(hits) > 1:
        out += "\n\n---\n**ì¶”ê°€ ê·¼ê±°**\n"
        for h in hits[1:3]:
            m = h.metadata
            out += f"- {m.get('ì¥','')} {m.get('ì ˆ','')} {m.get('í•­','')} â€” {m.get('ì œëª©','')}\n"
    return out


def answer_poly(q: str) -> str:
    if POLY.empty:
        return "ë‹¤ì˜ì–´ ë°ì´í„°(polysemy.csv)ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤."
    cands = [w for w in POLY["í‘œì œì–´"].unique() if isinstance(w, str) and w in q]
    if not cands:
        return "ì–´ë–¤ ë‹¨ì–´ì˜ ì—¬ëŸ¬ ëœ»ì„ ë¬»ëŠ”ì§€ ì•Œë ¤ ì£¼ì„¸ìš”. (ì˜ˆ: 'ë“¤ë‹¤ ë‹¤ì˜ì–´ ì•Œë ¤ì¤˜')"
    w = max(cands, key=len)
    rows = POLY[POLY["í‘œì œì–´"] == w]
    lines = [f"â€˜{w}â€™ì˜ ëœ»"]
    for _, r in rows.sort_values("ì˜ë¯¸ë²ˆí˜¸").iterrows():
        num = int(r["ì˜ë¯¸ë²ˆí˜¸"]) if str(r["ì˜ë¯¸ë²ˆí˜¸"]).isdigit() else r["ì˜ë¯¸ë²ˆí˜¸"]
        lines.append(f" {num}. {r['ëœ»']}  (ì˜ˆ: {r['ì˜ˆë¬¸']})")
    return "\n".join(lines)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í€´ì¦ˆ ë¬¸í•­ ìƒì„±ê¸°: vocab.csvì—ì„œ në¬¸í•­ ë½‘ê¸° (ë©”íƒ€ëŠ” [ìœ í˜•]ë§Œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import re

def clean_text(s: str) -> str:
    """ë³´ê¸°/ì§ˆë¬¸ ì•ë’¤ì˜ íŠ¹ìˆ˜ë¬¸ìë‚˜ ìˆ«ì ì œê±°"""
    if not isinstance(s, str):
        return ""
    s = s.strip()
    # '=', '-', 'â‘¥', 'â‘ '~'â‘©', ê´„í˜¸, ìˆ«ì ë“± ì œê±°
    s = re.sub(r'^[=\-\dâ‘´-â‘½â‘´â‘½\(\)\sÂ·\.\,]+', '', s)
    s = re.sub(r'[\sÂ·\.\,]+$', '', s)
    return s.strip()

# ==== ê³µí†µ: CSV ì•ˆì „ ë¡œë” ====
def _read_csv_expect(path: str, expected_cols: list[str]) -> pd.DataFrame:
    if not os.path.exists(path):
        return pd.DataFrame(columns=expected_cols)
    df = pd.read_csv(path)
    for c in expected_cols:
        if c not in df.columns: df[c] = ""
    return df[expected_cols].fillna("").astype(str)

# ==== í†µí•© ì–´íœ˜ í€´ì¦ˆ ====
def build_quiz_lexicon(df: pd.DataFrame, n: int) -> list[dict]:
    need = {"ìœ í˜•","ì–´íœ˜","ëœ»í’€ì´"}
    if not need.issubset(df.columns) or df.empty: return []
    base = df.dropna(subset=["ì–´íœ˜","ëœ»í’€ì´"]).copy().sample(frac=1.0)
    items = []
    for _, r in base.iterrows():
        q_word, correct = r["ì–´íœ˜"].strip(), r["ëœ»í’€ì´"].strip()
        cat, ex = r.get("ìœ í˜•","ì–´íœ˜"), str(r.get("ì˜ˆë¬¸","")).strip()
        if not q_word or not correct: continue
        same = base[(base["ìœ í˜•"]==cat) & (base["ëœ»í’€ì´"]!=correct)]["ëœ»í’€ì´"].unique().tolist()
        if len(same) < 3:
            same = base[base["ëœ»í’€ì´"]!=correct]["ëœ»í’€ì´"].unique().tolist()
        if len(same) < 3: continue
        choices = random.sample(same, 3) + [correct]
        random.shuffle(choices)
        items.append({"question": f"â€˜{q_word}â€™ì˜ ëœ»ìœ¼ë¡œ ê°€ì¥ ì•Œë§ì€ ê²ƒì€?",
                      "choices": choices, "answer": correct, "ex": ex, "meta": f"[{cat}]"})
        if len(items) >= n: break
    return items

# ==== ë„ì–´ì“°ê¸° ====
def build_quiz_spacing(n: int) -> list[dict]:
    df = _read_csv_expect("data/ë„ì–´ì“°ê¸°.csv", ["ìœ í˜•","ì •ë‹µ","ì˜¤ë‹µ"])
    items = []
    if df.empty: return items
    for _, r in df.sample(frac=1.0).iterrows():
        wrong, correct, cat = r["ì˜¤ë‹µ"], r["ì •ë‹µ"], r.get("ìœ í˜•","ë„ì–´ì“°ê¸°")
        if not wrong or not correct: continue
        other = df[df["ì •ë‹µ"]!=correct]["ì •ë‹µ"].unique().tolist()
        if len(other) < 3: continue
        choices = random.sample(other, 3) + [correct]
        random.shuffle(choices)
        items.append({"question": f"â€˜{wrong}â€™ì˜ ì˜¬ë°”ë¥¸ ë„ì–´ì“°ê¸°ëŠ”?",
                      "choices": choices, "answer": correct, "ex": "", "meta": f"[{cat}]"})
        if len(items) >= n: break
    return items

# ==== ë§ì¶¤ë²• ====
def build_quiz_orthography(n: int) -> list[dict]:
    df = _read_csv_expect("data/ë§ì¶¤ë²•.csv", ["ìœ í˜•","ì •ë‹µ ë‹¨ì–´","ì˜¤ë‹µ ë‹¨ì–´"])
    items = []
    if df.empty: return items
    rights = df["ì •ë‹µ ë‹¨ì–´"].unique().tolist()
    for _, r in df.sample(frac=1.0).iterrows():
        wrong, correct, cat = r["ì˜¤ë‹µ ë‹¨ì–´"], r["ì •ë‹µ ë‹¨ì–´"], r.get("ìœ í˜•","ë§ì¶¤ë²•")
        if not wrong or not correct: continue
        other = [x for x in rights if x != correct]
        if len(other) < 3: continue
        choices = random.sample(other, 3) + [correct]
        random.shuffle(choices)
        items.append({"question": f"â€˜{wrong}â€™ì˜ ì˜¬ë°”ë¥¸ í‘œê¸°ëŠ”?",
                      "choices": choices, "answer": correct, "ex": "", "meta": f"[{cat}]"})
        if len(items) >= n: break
    return items

# ==== ì™¸ë˜ì–´ ====
def build_quiz_loanword(n: int) -> list[dict]:
    df = _read_csv_expect("data/ì™¸ë˜ì–´.csv", ["ìœ í˜•","ì •ë‹µ ì™¸ë˜ì–´","ì˜¤ë‹µ ì™¸ë˜ì–´"])
    items = []
    if df.empty: return items
    rights = df["ì •ë‹µ ì™¸ë˜ì–´"].unique().tolist()
    for _, r in df.sample(frac=1.0).iterrows():
        wrong, correct, cat = r["ì˜¤ë‹µ ì™¸ë˜ì–´"], r["ì •ë‹µ ì™¸ë˜ì–´"], r.get("ìœ í˜•","ì™¸ë˜ì–´")
        if not wrong or not correct: continue
        other = [x for x in rights if x != correct]
        if len(other) < 3: continue
        choices = random.sample(other, 2) + [wrong] + [correct]
        random.shuffle(choices)
        items.append({"question": "ë‹¤ìŒ ì¤‘ ì˜¬ë°”ë¥¸ ì™¸ë˜ì–´ í‘œê¸°ëŠ”?",
                      "choices": choices, "answer": correct, "ex": "", "meta": f"[{cat}]"})
        if len(items) >= n: break
    return items

# ==== í‘œì¤€ë°œìŒë²• ====
def build_quiz_pron(n: int) -> list[dict]:
    df = _read_csv_expect("data/í‘œì¤€ë°œìŒë²•.csv", ["ìœ í˜•","ë‹¨ì–´","í‘œì¤€ ë°œìŒ"])
    items = []
    if df.empty: return items
    for _, r in df.sample(frac=1.0).iterrows():
        word, correct, cat = r["ë‹¨ì–´"], r["í‘œì¤€ ë°œìŒ"], r.get("ìœ í˜•","í‘œì¤€ë°œìŒë²•")
        if not word or not correct: continue
        other = df[df["í‘œì¤€ ë°œìŒ"]!=correct]["í‘œì¤€ ë°œìŒ"].unique().tolist()
        if len(other) < 3: continue
        choices = random.sample(other, 3) + [correct]
        random.shuffle(choices)
        items.append({"question": f"â€˜{word}â€™ì˜ í‘œì¤€ ë°œìŒì€?",
                      "choices": choices, "answer": correct, "ex": "", "meta": f"[{cat}]"})
        if len(items) >= n: break
    return items

# ==== ë¡œë§ˆìí‘œê¸°ë²• ====
def build_quiz_romaja(n: int) -> list[dict]:
    df = _read_csv_expect("data/ë¡œë§ˆìí‘œê¸°ë²•.csv", ["ìœ í˜•","ë‹¨ì–´","ë¡œë§ˆì"])
    items = []
    if df.empty: return items
    for _, r in df.sample(frac=1.0).iterrows():
        word, correct, cat = r["ë‹¨ì–´"], r["ë¡œë§ˆì"], r.get("ìœ í˜•","ë¡œë§ˆìí‘œê¸°ë²•")
        if not word or not correct: continue
        other = df[df["ë¡œë§ˆì"]!=correct]["ë¡œë§ˆì"].unique().tolist()
        if len(other) < 3: continue
        choices = random.sample(other, 3) + [correct]
        random.shuffle(choices)
        items.append({"question": f"â€˜{word}â€™ì˜ ë¡œë§ˆì í‘œê¸°ëŠ”?",
                      "choices": choices, "answer": correct, "ex": "", "meta": f"[{cat}]"})
        if len(items) >= n: break
    return items

# ==== í‘œì¤€ì–´ê·œì • (ìœ í˜•/ë‹¨ì–´) ====
def build_quiz_standard_rule(n: int) -> list[dict]:
    df = _read_csv_expect("data/í‘œì¤€ì–´ê·œì •.csv", ["ìœ í˜•","ë‹¨ì–´"])
    items = []
    if df.empty: return items
    kinds = df["ìœ í˜•"].dropna().unique().tolist()
    if len(kinds) < 4 and kinds:
        while len(kinds) < 4: kinds.append(random.choice(kinds))
    for _, r in df.sample(frac=1.0).iterrows():
        word, correct = r["ë‹¨ì–´"], r["ìœ í˜•"]
        if not word or not correct: continue
        wrongs = [k for k in kinds if k != correct]
        if len(wrongs) < 3: continue
        choices = random.sample(wrongs, 3) + [correct]
        random.shuffle(choices)
        items.append({"question": f"â€˜{word}â€™ì€(ëŠ”) ì–´ë–¤ í‘œì¤€ì–´ ê·œì •ì— í•´ë‹¹í• ê¹Œìš”?",
                      "choices": choices, "answer": correct, "ex": "", "meta": "[í‘œì¤€ì–´ê·œì •]"})
        if len(items) >= n: break
    return items

# ==== ì „ì²´ í˜¼í•© í€´ì¦ˆ ====
def build_all_quiz_items(total: int = 10) -> list[dict]:
    per = max(1, total // 6)   # ëŒ€ëµ ê· ë“±
    items = []
    items += build_quiz_lexicon(VOCAB, n=per*2)   # ì–´íœ˜ ë¹„ì¤‘ ì¡°ê¸ˆ ë”
    items += build_quiz_spacing(per)
    items += build_quiz_orthography(per)
    items += build_quiz_loanword(per)
    items += build_quiz_pron(per)
    items += build_quiz_romaja(per)
    items += build_quiz_standard_rule(per)
    random.shuffle(items)
    return items[:total]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# íƒ­ UI: ì‹œí—˜ ì†Œê°œ Â· ì§ˆë¬¸í•˜ê¸° | í€´ì¦ˆ | í•™ìŠµí•˜ê¸° | ì˜¤ë‹µë…¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tab_intro_ask, tab_quiz, tab_learn, tab_wrong = st.tabs(
    ["ğŸ¥° ì‹œí—˜ ì†Œê°œ Â· ğŸ§ ì§ˆë¬¸í•˜ê¸°", "ğŸ¤— í€´ì¦ˆ í’€ê¸°", "ğŸ“š í•™ìŠµí•˜ê¸°", "ğŸ“˜ ì˜¤ë‹µë…¸íŠ¸"]
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) ì‹œí—˜ ì†Œê°œ Â· ì§ˆë¬¸í•˜ê¸° íƒ­ â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_intro_ask:
    # â”€â”€ (A) ì‹œí—˜ ì†Œê°œ ì„¹ì…˜ â”€â”€
    st.markdown("<h2>ğŸ“˜ KBS í•œêµ­ì–´ëŠ¥ë ¥ì‹œí—˜ ì†Œê°œ</h2>", unsafe_allow_html=True)

    with st.expander("ğŸŒ¾ í•œëŠ¥ì‹œ! ì™œ í•„ìš”í•œê°€ìš”?", expanded=True):
        st.markdown(
            """
**ì…í•™ / ì·¨ì—… / ìŠ¹ì§„ì— ìœ ë¦¬í•©ë‹ˆë‹¤.**
- íŠ¹ëª©ê³  ì§„í•™ ë° ëŒ€í•™ ì…í•™  
- KBSÂ·EBSÂ·ê²½í–¥ì‹ ë¬¸ ë“± ì–¸ë¡ ì‚¬ ì·¨ì—…  
- ìš°ë¦¬ì€í–‰Â·GSí™ˆì‡¼í•‘ ë“± ë¯¼ê°„ê¸°ì—… ì·¨ì—…  
- ê±´ê°•ë³´í—˜ê³µë‹¨Â·í•œêµ­ì „ë ¥ ë“± ê³µê³µê¸°ê´€ ì·¨ì—…  
- ê²½ì°°ì²­ ë“± ìŠ¹ì§„ì—ì„œ â€˜ê²°ì •ì  ê°€ì‚°ì â€™ í™•ë³´
            """
        )

    # â”€â”€ (B) ì‹œí—˜ êµ¬ì„± ì„¹ì…˜ â”€â”€
    with st.expander("ğŸ¹ ì‹œí—˜ êµ¬ì„± ë° ì˜ì—­", expanded=True):
        # ì„¤ëª… ë¬¸ì¥
        st.markdown(
            """
ë³¸ ì‹œí—˜ì€ ê°ê´€ì‹ **5ì§€ ì„ ë‹¤í˜•**ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì´ **100ë¬¸í•­**ì…ë‹ˆë‹¤.  
ì•„ë˜ëŠ” ì‹œê°„ëŒ€ë³„ ì˜ì—­ êµ¬ì„±í‘œì…ë‹ˆë‹¤.
            """
        )

        # í‘œ (ìƒ‰ìƒ ì œê±° + ì¤„ë°”ê¿ˆ ë°˜ì˜)
        st.markdown(
            """
<style>
.exam-table {
  border-collapse: collapse;
  width: 100%;
  margin-top: 10px;
  font-size: 16px;
  text-align: center;
}
.exam-table th {
  background-color: #f8f9fa;
  padding: 10px;
  border-bottom: 2px solid #ddd;
}
.exam-table td {
  padding: 10px;
  border-bottom: 1px solid #eee;
}
</style>

<table class="exam-table">
  <tr>
    <th>ì‹œê°„ëŒ€</th>
    <th>ì˜ì—­</th>
    <th>ë¬¸í•­ ìˆ˜</th>
  </tr>
  <tr>
    <td>10:00~10:25<br>(25ë¶„)</td>
    <td>ë“£ê¸°Â·ë§í•˜ê¸°</td>
    <td>15ë¬¸í•­</td>
  </tr>
  <tr>
    <td>10:25~12:00<br>(95ë¶„)</td>
    <td>ì“°ê¸° Â· ì°½ì•ˆ Â· ì½ê¸° Â· êµ­ì–´ë¬¸í™”<br>Â· ì–´íœ˜ Â· ì–´ë²•</td>
    <td>85ë¬¸í•­<br>(ì“°ê¸° 5 + ì°½ì•ˆ 10 + ì½ê¸° 30 + êµ­ì–´ë¬¸í™” 10 + ì–´íœ˜Â·ì–´ë²• 30)</td>
  </tr>
</table>
            """,
            unsafe_allow_html=True,
        )

    # â”€â”€ (C) ì˜ì—­ë³„ ì¶œì œ ê²½í–¥ â”€â”€
    with st.expander("ğŸ¡ ì˜ì—­ë³„ ì¶œì œ ê²½í–¥"):
        st.markdown(
            """
<div style="font-size:15px; line-height:1.8;">
  <p><b>ğŸ« ë“£ê¸°Â·ë§í•˜ê¸°</b></p>
  <ul style="margin-left:12px;">
     âœ”ï¸ ê·¸ë¦¼Â·ì¥ë©´Â·ë¼ë””ì˜¤ ë“£ê³  í•µì‹¬ ë‚´ìš© íŒŒì•…
     âœ”ï¸ ê³ ì „/ìš°í™”/ì‹œ ì²­ì·¨ í›„ ì˜ë¯¸ ë° ê°ì • ì¶”ë¡ 
     âœ”ï¸ ëŒ€í™”Â·ë°œí‘œ ë“£ê³  ë§í•˜ê¸° ë°©ì‹Â·í™”ìì˜ íƒœë„ íŒŒì•…
  </ul>

  <p><b>ğŸ® ì–´íœ˜</b></p>
  <ul style="margin-left:12px;">
     âœ”ï¸ ê³ ìœ ì–´Â·í•œìì–´ ëœ»/í‘œê¸° êµ¬ë¶„
     âœ”ï¸ ì–´íœ˜ ê´€ê³„(ìœ ì˜/ë°˜ì˜/ìƒí•˜)Â·ì†ë‹´Â·ê´€ìš©êµ¬
     âœ”ï¸ ì™¸ë˜ì–´Â·í•œìì–´ì˜ ì˜¬ë°”ë¥¸ ìš°ë¦¬ë§ ì“°ì„
  </ul>

  <p><b>ğŸ° ì–´ë²•</b></p>
  <ul style="margin-left:12px;">
     âœ”ï¸ ë§ì¶¤ë²•/í‘œì¤€ì–´/ë°œìŒ/í‘œê¸°ë²• êµ¬ë¶„
     âœ”ï¸ ë¬¸ì¥ í˜¸ì‘ ë° ì˜ëª»ëœ í‘œí˜„ ì§„ë‹¨Â·ìˆ˜ì •
  </ul>

  <p><b>ğŸ¯ ì“°ê¸°</b></p>
  <ul style="margin-left:12px;">
     âœ”ï¸ ê³„íšÂ·ê°œìš” ìˆ˜ì •, ìë£Œ í™œìš©, ê¸€ ê³ ì³ì“°ê¸°
  </ul>
</div>
            """,
            unsafe_allow_html=True,
        )

    # â”€â”€ (B) ì§ˆë¬¸í•˜ê¸° ì„¹ì…˜ â”€â”€
    st.markdown(
        "<p style='font-size:18px; font-weight:600;'>ğŸ² ì €ì—ê²Œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš© ğŸŒŸ!</p>"
        "<p style='font-size:14px; color:#555; margin-top:-10px; margin-bottom:-8px;'>"
        "(e.g. 'êµê°ì‚´ìš°'ì˜ ëœ»ì´ ê¶ê¸ˆí•´ìš”,<br>"
        "'ëŠ‘ë§‰ì—¼'ì˜ í‘œì¤€ ë°œìŒì„ ì•Œë ¤ì£¼ì„¸ìš”,<br>"
        "ìë£Œì—ì„œ ~ ëŠ” ì–´ë””ì— ë‚˜ì˜¤ë‚˜ìš”?)"
        "</p>",
        unsafe_allow_html=True
    )

    # ì…ë ¥ì°½ (ì˜ˆì‹œ ë¬¸êµ¬ì™€ ë”± ë¶™ê²Œ)
    user_q = st.text_input("", placeholder="ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” ğŸ˜Š")

    # ì‘ì› ë¬¸êµ¬ (2ì¤„ ê³µë°± í›„)
    st.markdown(
        "<br><br>"
        "<p style='color:gray; font-size:20px; font-weight:600; text-align:center;'>"
        "íŒŒì´íŒ…!!! ìŒ¤ì€ ì—¬ëŸ¬ë¶„ì˜ ì‹œí—˜ì„ ì‘ì›í•©ë‹ˆë‹¤~! ğŸ¥°"
        "</p>",
        unsafe_allow_html=True
    )

    if user_q:
        kind = intent(user_q)
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘â€¦"):
            try:
                if kind == "vocab":
                    ans = answer_vocab(user_q)
                elif kind == "rule":
                    ans = answer_rule(user_q)
                elif kind == "poly":
                    ans = answer_poly(user_q)
                else:
                    ans = run_rag(user_q)

                st.write(ans)

                if kind == "rag" and retriever is not None:
                    with st.expander("ğŸ” ê·¼ê±° ë³´ê¸°(ì—…ë¡œë“œ ìë£Œì—ì„œ ì¶”ì¶œ)"):
                        docs = retriever.invoke(user_q)
                        for i, d in enumerate(docs, 1):
                            st.markdown(f"**ê·¼ê±° {i}**")
                            st.code((d.page_content or "")[:800])
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# ë‚˜ë¨¸ì§€ íƒ­(tab_quiz, tab_learn, tab_wrong)ì€
# ê¸°ì¡´ì— ì“°ë˜ ì½”ë“œ ë¸”ë¡ì„ ê·¸ëŒ€ë¡œ ì´ì–´ì„œ ì‚¬ìš©í•˜ì„¸ìš”.

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í€´ì¦ˆ íƒ­ (í™•ì¥ ë²„ì „: ì œì¶œ â†’ ê²°ê³¼ â†’ ìƒˆ í€´ì¦ˆ ë²„íŠ¼ ìˆœì„œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_quiz:
    st.markdown("â¤ï¸ì¨”ë€! **ëœë¤ ì¢…í•© í€´ì¦ˆ**ë¥¼ í’€ì–´ë³´ì„¸ìš”!ğŸ˜˜")

    # ì²˜ìŒ ë¡œë“œ ì‹œ data/ ëª¨ë“  CSV ê¸°ë°˜ìœ¼ë¡œ í˜¼í•© í€´ì¦ˆ ìƒì„±
    if "quiz_items" not in st.session_state or not st.session_state.quiz_items:
        st.session_state.quiz_items = build_all_quiz_items(total=10)
        st.session_state.quiz_submitted = False
        st.session_state.quiz_score = 0

    if not st.session_state.quiz_items:
        st.info("í€´ì¦ˆë¥¼ ë§Œë“¤ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. data/ í´ë”ì˜ CSVë“¤ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
    else:
        # ë¬¸í•­ ë Œë”ë§
        answers = {}
        for i, item in enumerate(st.session_state.quiz_items):
            st.markdown(
                f"**Q{i+1}. {item['question']}**  \n<small>{item['meta']}</small>",
                unsafe_allow_html=True
            )
            key = f"quiz_q_{i}"
            choice = st.radio(
                "ë³´ê¸°",
                options=[clean_text(c) for c in item["choices"]],
                index=None,
                key=key,
                label_visibility="collapsed"
            )
            answers[i] = choice
            st.divider()

        # ë²„íŠ¼ 1ì„¸íŠ¸ë§Œ ë°°ì¹˜
        submitted = st.button("âœ… ì œì¶œ", key="quiz_submit_btn", type="primary", use_container_width=True)
        new_quiz  = st.button("ğŸ”„ ìƒˆ í€´ì¦ˆ ì¶œì œ", key="quiz_new_btn", use_container_width=True)

        # ì œì¶œ â†’ ì±„ì /ì˜¤ë‹µë…¸íŠ¸ ì €ì¥/í•´ì„¤
        if submitted:
            score = 0
            results = []
            wrong_items = []

            for i, item in enumerate(st.session_state.quiz_items):
                sel = answers.get(i)
                ok = (sel == item["answer"])
                score += int(ok)
                results.append((ok, sel, item))
                if not ok:
                    wrong_items.append({
                        "ë¬¸í•­": item["question"],
                        "ì„ íƒí•œ ë‹µ": sel if sel is not None else "(ë¬´ì‘ë‹µ)",
                        "ì •ë‹µ": item["answer"],
                        "ì˜ˆë¬¸": item.get("ex", "")
                    })

            st.session_state.quiz_score = score
            st.session_state.quiz_submitted = True
            st.session_state["wrong_items"] = wrong_items

            st.success(f"ì ìˆ˜: **{score} / {len(st.session_state.quiz_items)}**")
            with st.expander("ì •ë‹µ ë° í•´ì„¤ ë³´ê¸°"):
                for i, (ok, sel, item) in enumerate(results, start=1):
                    icon = "âœ…" if ok else "âŒ"
                    sel_txt = sel if sel is not None else "(ë¬´ì‘ë‹µ)"
                    st.markdown(f"**{icon} Q{i}. {item['question']}**")
                    st.write(f"- ì„ íƒ: {sel_txt}")
                    st.write(f"- ì •ë‹µ: {item['answer']}")
                    if item.get("ex"):
                        st.write(f"- ì˜ˆë¬¸: {item['ex']}")
                    st.write("---")

        # ìƒˆ í€´ì¦ˆ
        if new_quiz:
            st.session_state.quiz_items = build_all_quiz_items(total=10)  # ì´ ë¬¸í•­ ìˆ˜
            st.session_state.quiz_submitted = False
            st.session_state.quiz_score = 0
            st.rerun()

# ========== í•™ìŠµ íƒ­ í—¬í¼ ==========
def init_study():
    """í•™ìŠµ ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”"""
    if "study" not in st.session_state:
        st.session_state.study = {
            "today_goal": {"lex": 10, "rule": 5, "poly": 3},
            "seen_ids": [],
            "bookmarks": [],
            "leitner": {"1": [], "2": [], "3": []},
            "progress": {
                "date": pd.Timestamp.today().date().isoformat(),
                "lex": 0, "rule": 0, "poly": 0
            }
        }

@st.cache_data
def rules_df():
    """rules.json â†’ DataFrame"""
    return pd.DataFrame(RULES) if RULES else pd.DataFrame(columns=["ê·œì •ëª…","í•­ëª©","ì„¤ëª…","ì˜ˆì‹œ"])

def show_rule_card(idx: int) -> int:
    """ê·œì • í•™ìŠµìš© ì¹´ë“œ ë·° + ì´ì „/ë‹¤ìŒ/ë¶ë§ˆí¬"""
    df = rules_df()
    if df.empty:
        st.info("rules.jsonì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return 0
    idx = max(0, min(idx, len(df)-1))
    r = df.iloc[idx]
    st.markdown(f"### ã€”{r.get('ê·œì •ëª…','ê·œì •')}ã€• {r.get('í•­ëª©','')}")
    st.write(r.get("ì„¤ëª…",""))
    ex = r.get("ì˜ˆì‹œ","")
    if isinstance(ex, str) and ex.strip():
        st.code(ex)
    c1, c2, c3 = st.columns(3)
    if c1.button("â¬…ï¸ ì´ì „", key=f"rule_prev_{idx}"):
        idx -= 1
    if c2.button("â­ ì¤‘ìš” í‘œì‹œ", key=f"rule_star_{idx}"):
        st.session_state.study["bookmarks"].append(("rule", idx))
        st.toast("ì¤‘ìš” í•­ëª©ìœ¼ë¡œ ì €ì¥í–ˆì–´ìš”!", icon="â­")
    if c3.button("ë‹¤ìŒ â¡ï¸", key=f"rule_next_{idx}"):
        idx += 1
    return idx

def flash_lex(df: pd.DataFrame):
    """ì–´íœ˜ í”Œë˜ì‹œì¹´ë“œ: ì•(ì–´íœ˜) / ë’¤(ëœ»í’€ì´Â·ì˜ˆë¬¸)"""
    if df.empty:
        st.info("ì–´íœ˜ CSVê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return
    if "lex_idx" not in st.session_state:
        st.session_state.lex_idx = 0
    i = st.session_state.lex_idx % len(df)
    row = df.iloc[i]
    front = f"**ã€”{row.get('ìœ í˜•','ì–´íœ˜')}ã€• {row.get('ì–´íœ˜','(ì–´íœ˜)')}**"
    ex = row.get("ì˜ˆë¬¸","")
    back_lines = [f"ëœ»: {row.get('ëœ»í’€ì´','-')}"]
    if isinstance(ex, str) and ex.strip():
        back_lines.append(f"\nì˜ˆë¬¸: {ex}")
    back = "\n".join(back_lines)

    st.markdown(front)
    if st.toggle("ì •ë‹µ ë³´ê¸°", key=f"lex_show_{i}"):
        st.write(back)

    c1, c2, c3 = st.columns(3)
    if c1.button("í‹€ë¦¼", key=f"lex_wrong_{i}"):
        st.session_state.study["leitner"]["1"].append(("lex", i))
        st.session_state.lex_idx += 1
    if c2.button("ì •ë‹µ", key=f"lex_right_{i}"):
        st.session_state.study["progress"]["lex"] += 1
        st.session_state.study["leitner"]["2"].append(("lex", i))
        st.session_state.lex_idx += 1
    if c3.button("ê±´ë„ˆë›°ê¸°", key=f"lex_skip_{i}"):
        st.session_state.lex_idx += 1

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“š í•™ìŠµí•˜ê¸° íƒ­
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_learn:
    init_study()
    S = st.session_state.study
    G = S["today_goal"]; P = S["progress"]

    # â”€â”€ ê·œì • í•™ìŠµ â”€â”€
    with st.expander("ğŸ§‹ ê·œì • í•™ìŠµ ğŸ¥‚", expanded=True):
        if "rule_idx" not in st.session_state:
            st.session_state.rule_idx = 0

        # ê·œì • ì¹´ë“œ ë³´ì—¬ì£¼ê¸° (Ellipsis ì¶œë ¥ ì›ì¸ ì œê±°: ì ˆëŒ€ ë¹ˆ ë¸”ë¡/â€¦ ì‚¬ìš© X)
        st.session_state.rule_idx = show_rule_card(st.session_state.rule_idx)

        if st.button("í•™ìŠµ ì™„ë£Œ(ê·œì • 1 ì¦ê°€)", key="rule_done_learn"):
            st.session_state.study["progress"]["rule"] += 1
            st.toast("ê·œì • 1ê°œ í•™ìŠµ ì™„ë£Œ!", icon="âœ…")

    # â”€â”€ ê·œì • JSON ì—…ë¡œë“œ (ì„ íƒ) â”€â”€
    with st.expander("ğŸˆ ê·œì • JSON ì—…ë¡œë“œ (ì„ íƒì‚¬í•­ì…ë‹ˆë‹¤) ğŸŒ½"):
        up = st.file_uploader(
            "rules.json ì—…ë¡œë“œ",
            type=["json"],
            key="rules_json_uploader_learn"
        )
        if up is not None:
            with open(RULES_JSON_PATH, "wb") as f:
                f.write(up.read())
            st.cache_data.clear()
            st.cache_resource.clear()
            st.success("ê·œì • ë°ì´í„°ê°€ ìƒˆë¡œ ì ìš©ë˜ì—ˆì–´ìš”. ì ì‹œ í›„ ìë™ìœ¼ë¡œ ë°˜ì˜ë©ë‹ˆë‹¤.")
            st.rerun()

    # â”€â”€ ì–´íœ˜ í”Œë˜ì‹œì¹´ë“œ â”€â”€
    with st.expander("ğŸ‡ ì–´íœ˜ í”Œë˜ì‹œì¹´ë“œ ğŸ“", expanded=True):
        flash_lex(VOCAB)

    # ë¯¸ë‹ˆ í…ŒìŠ¤íŠ¸ (ë°©ê¸ˆ í•™ìŠµí•œ ë§¥ë½ìœ¼ë¡œ 5ë¬¸í•­)
    with st.expander("ğŸ‹ ë¯¸ë‹ˆ í…ŒìŠ¤íŠ¸ (ë°©ê¸ˆ í•™ìŠµí•œ ë§¥ë½ìœ¼ë¡œ 5ë¬¸í•­!) ğŸ«"):
        mini_items = build_all_quiz_items(total=5)  # ê¸°ì¡´ ë¹Œë” ì¬ì‚¬ìš©
        mini_answers = {}
        for i, q in enumerate(mini_items):
            st.markdown(f"**Q{i+1}. {q['question']}**  \n<small>{q['meta']}</small>", unsafe_allow_html=True)
            mini_answers[i] = st.radio("ë³´ê¸°", q["choices"], index=None, key=f"mini_{i}", label_visibility="collapsed")
            st.divider()
        if st.button("ì±„ì ", key="mini_grade_btn"):
            score, wrong = 0, []
            for i, q in enumerate(mini_items):
                sel = mini_answers[i]
                ok = sel == q["answer"]; score += int(ok)
                if not ok:
                    wrong.append({
                        "ë¬¸í•­": q["question"],
                        "ì„ íƒí•œ ë‹µ": sel if sel is not None else "(ë¬´ì‘ë‹µ)",
                        "ì •ë‹µ": q["answer"],
                        "ì˜ˆë¬¸": q.get("ex","")
                    })
            st.success(f"ì ìˆ˜: {score} / {len(mini_items)}")
            # ì˜¤ë‹µë…¸íŠ¸ì— ëˆ„ì 
            st.session_state["wrong_items"] = st.session_state.get("wrong_items", []) + wrong

    # ì˜¤ë‹µ ë³µìŠµ (ê°„ë‹¨ Leitner)
    with st.expander("ğŸŠ ì˜¤ë‹µ ë³µìŠµ ğŸ’"):
        boxes = st.session_state.study["leitner"]
        st.write({f"ë°•ìŠ¤ {k}": len(v) for k, v in boxes.items()})
        # ê°„ë‹¨: ë°•ìŠ¤1 â†’ 2 â†’ 3 ìˆœ
        pool = boxes["1"] or boxes["2"] or boxes["3"]
        if not pool:
            st.info("ë³µìŠµí•  ì¹´ë“œê°€ ì—†ì–´ìš”. í€´ì¦ˆ/í•™ìŠµì—ì„œ í‹€ë¦° í•­ëª©ì´ ìƒê¸°ë©´ ì—¬ê¸°ì— ìŒ“ì…ë‹ˆë‹¤.")
        else:
            src, i = pool[0]  # ("lex", index) í˜•íƒœ
            if src == "lex" and not VOCAB.empty:
                r = VOCAB.iloc[i]
                st.markdown(f"**ã€”{r.get('ìœ í˜•','ì–´íœ˜')}ã€• {r.get('ì–´íœ˜','')}**")
                if st.button("ì •ë‹µ (ìƒìœ„ ë°•ìŠ¤ë¡œ ì´ë™)", key=f"leit_up_{i}"):
                    if pool is boxes["1"]:
                        boxes["2"].append(pool.pop(0))
                    elif pool is boxes["2"]:
                        boxes["3"].append(pool.pop(0))
                    else:
                        pool.pop(0)
                if st.button("ì˜¤ë‹µ (1ë‹¨ê³„ë¡œ)", key=f"leit_reset_{i}"):
                    if pool:
                        pair = pool.pop(0)
                    boxes["1"].append(("lex", i))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì˜¤ë‹µë…¸íŠ¸ íƒ­
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_wrong:
    st.markdown("ğŸ“˜ **ì˜¤ë‹µë…¸íŠ¸** (í‹€ë¦° ë¬¸ì œë¥¼ ì´ê³³ì—ì„œ ë³µìŠµí•´ë³´ì•„ìš”!)")

    # ë¹„ìš°ê¸° ë²„íŠ¼ (ì„ íƒ)
    col1, col2 = st.columns([1,3])
    with col1:
        if st.button("ğŸ§¹ ì˜¤ë‹µë…¸íŠ¸ ë¹„ìš°ê¸°", key="clear_wrong_btn"):
            st.session_state["wrong_items"] = []
            st.success("ì˜¤ë‹µë…¸íŠ¸ë¥¼ ë¹„ì› ì–´ìš”!")

    wrong_items = st.session_state.get("wrong_items", [])

    if not wrong_items:
        st.info("ì•„ì§ ì˜¤ë‹µì´ ì—†ìŠµë‹ˆë‹¤! í€´ì¦ˆë¥¼ ë¨¼ì € í’€ì–´ë³´ì„¸ìš” ğŸ˜")
    else:
        for i, w in enumerate(wrong_items, start=1):
            st.markdown(f"**Q{i}. {w.get('ë¬¸í•­','(ë¬¸í•­ ì •ë³´ ì—†ìŒ)')}**")
            st.write(f"- ì„ íƒí•œ ë‹µ: {w.get('ì„ íƒí•œ ë‹µ', '(ë¬´ì‘ë‹µ)')}")
            st.write(f"- ì •ë‹µ: {w.get('ì •ë‹µ', '-')}")
            ex = w.get("ì˜ˆë¬¸", "")
            if isinstance(ex, str) and ex.strip():
                st.write(f"- ì˜ˆë¬¸: {ex}")
            st.divider()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ì´ë“œë°”: ìƒíƒœ/í™•ì¥ ì•ˆë‚´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.markdown("### ìƒíƒœ")
    st.write(f"- ì–´íœ˜ ì‚¬ì „ ë¡œë“œ: {'âœ…' if not VOCAB.empty else 'âŒ'}")
    st.write(f"- ê·œì • ì¹´ë“œ ë¡œë“œ: {'âœ…' if RULES else 'âŒ'}")
    # ê¸°ì¡´: st.write(f"- ë‹¤ì˜ì–´ ì¹´ë“œ ë¡œë“œ: {'âœ…' if not POLY.empty else 'âŒ'}")
    poly_ok = isinstance(POLY, pd.DataFrame) and not POLY.empty
    st.write(f"- ë‹¤ì˜ì–´ ì¹´ë“œ ë¡œë“œ: {'âœ…' if poly_ok else 'âŒ'}")
    st.write(f"- ì—…ë¡œë“œ ìë£Œ ìƒ‰ì¸: {'âœ…' if retriever is not None else 'âŒ'}")
    st.divider()
    st.markdown("### ì‚¬ìš©ë²•")
    st.markdown("- ì–´íœ˜: `êµê°ì‚´ìš° ëœ»`, `ì„ì”¨ë…„ìŠ¤ëŸ½ë‹¤ ì˜ë¯¸`")
    st.markdown("- ê·œì •: `ê°™ì´ ë„ì–´ì“°ê¸°`, `ê°’ì´ ë°œìŒ`, `í”¼ì í‘œê¸°`")
    st.markdown("- ë‹¤ì˜ì–´: `ë“¤ë‹¤ ë‹¤ì˜ì–´`, `ë‹¬ë‹¤ ì—¬ëŸ¬ ëœ»`, `ì¹˜ë¥´ë‹¤ ëœ»ë“¤`")
    st.markdown("- í€´ì¦ˆ: íƒ­ì—ì„œ **ìƒˆ í€´ì¦ˆ ì¶œì œ â†’ ì œì¶œ**")
    st.markdown("- ì—…ë¡œë“œ RAG: íŒŒì¼ ì˜¬ë¦¬ê³  ììœ  ì§ˆì˜")







